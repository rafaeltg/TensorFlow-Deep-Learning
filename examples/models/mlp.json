{
    "model": {
        "class_name": "MLP",
        "config": {
            "layers": [
                32,
                16
            ],
            "dropout": [
                0.1,
                0.1
            ],
            "l2_reg": [
                1e-05,
                1e-05
            ],
            "num_epochs": 200,
            "name": "mlp",
            "activation": [
                "relu",
                "relu"
            ],
            "opt": "adam",
            "batch_size": 32,
            "out_activation": "linear",
            "loss_func": "mse",
            "learning_rate": 0.001,
            "seed": -1,
            "l1_reg": [
                1e-05,
                1e-05
            ],
            "verbose": 0
        }
    },
    "weights": "models/mlp.h5"
}